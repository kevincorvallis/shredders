# ðŸ—„ï¸ Database Setup Guide

Complete guide to set up AWS RDS PostgreSQL database for the mountain scraper.

## ðŸ“‹ Prerequisites

- AWS Account with billing enabled
- AWS CLI installed: `brew install awscli` (macOS)
- PostgreSQL client installed: `brew install postgresql` (macOS)
- Node.js and npm installed

## ðŸš€ Quick Start

### 1. Configure AWS Credentials

```bash
aws configure
# Enter your AWS Access Key ID, Secret Access Key, and region (us-west-2 recommended)
```

### 2. Create RDS Database

```bash
npm run db:setup-aws
```

This script will:
- âœ… Create a PostgreSQL 15.4 RDS instance (db.t3.micro - free tier eligible)
- âœ… Enable encryption and backups
- âœ… Generate a secure password
- âœ… Save connection details to `.env.local`
- â±ï¸ Takes 5-10 minutes

**Output:**
```
ðŸ”ï¸ Setting up AWS RDS PostgreSQL for Mountain Scraper

Configuration:
  Instance ID: shredders-mountain-data
  Database: mountains
  Username: shredders_admin
  Region: us-west-2
  Instance Class: db.t3.micro

âœ… Database instance created and available

Connection String:
  postgresql://shredders_admin:****@******.rds.amazonaws.com:5432/mountains?sslmode=require
```

### 3. Create Database Schema

```bash
npm run db:setup
```

This runs the SQL schema file and creates:
- âœ… `mountain_status` table (stores scraped data)
- âœ… `scraper_runs` table (tracks scraper executions)
- âœ… Views for latest status and statistics
- âœ… Functions for cleanup and history queries
- âœ… Indexes for performance

### 4. Add Environment Variables to Vercel

```bash
# Production
vercel env add DATABASE_URL production
# Paste the connection string from .env.local

# Preview (optional)
vercel env add DATABASE_URL preview

# Development (uses .env.local automatically)
```

### 5. Deploy to Vercel

```bash
vercel --prod
```

## ðŸ“Š Database Schema

### Tables

#### `mountain_status`
Stores scraped mountain status data.

| Column | Type | Description |
|--------|------|-------------|
| id | UUID | Primary key |
| mountain_id | VARCHAR(50) | Mountain identifier (e.g., "baker") |
| is_open | BOOLEAN | Whether mountain is open |
| percent_open | INTEGER | Percentage of terrain open (0-100) |
| lifts_open | INTEGER | Number of lifts operating |
| lifts_total | INTEGER | Total number of lifts |
| runs_open | INTEGER | Number of runs open |
| runs_total | INTEGER | Total number of runs |
| message | TEXT | Status message from resort |
| scraped_at | TIMESTAMP | When data was scraped |
| source_url | TEXT | Source URL |

#### `scraper_runs`
Tracks scraper execution metadata.

| Column | Type | Description |
|--------|------|-------------|
| id | UUID | Primary key |
| run_id | VARCHAR(100) | Unique run identifier |
| total_mountains | INTEGER | Number of mountains to scrape |
| successful_count | INTEGER | Successfully scraped |
| failed_count | INTEGER | Failed to scrape |
| started_at | TIMESTAMP | Run start time |
| completed_at | TIMESTAMP | Run completion time |
| duration_ms | INTEGER | Duration in milliseconds |
| status | VARCHAR(20) | 'running', 'completed', or 'failed' |
| triggered_by | VARCHAR(50) | 'manual', 'cron', 'github-actions' |

### Views

#### `latest_mountain_status`
Shows the most recent status for each mountain.

```sql
SELECT * FROM latest_mountain_status;
```

#### `scraper_stats`
Daily aggregated statistics for scraper runs.

```sql
SELECT * FROM scraper_stats;
```

### Functions

#### `cleanup_old_mountain_status()`
Deletes data older than 90 days.

```sql
SELECT cleanup_old_mountain_status();
```

#### `get_mountain_history(mountain_id, days)`
Get historical data for a mountain.

```sql
SELECT * FROM get_mountain_history('baker', 7);
```

## ðŸ”§ Configuration

### Environment Variables

Add to `.env.local` (auto-generated by setup script):

```bash
# AWS RDS PostgreSQL Connection
DATABASE_URL=postgresql://user:password@host:5432/mountains?sslmode=require
DB_HOST=your-instance.rds.amazonaws.com
DB_PORT=5432
DB_NAME=mountains
DB_USER=shredders_admin
DB_PASSWORD=your-secure-password
```

### Custom Configuration

Edit `scripts/setup-aws-database.sh` before running:

```bash
# Change instance size
DB_INSTANCE_CLASS=db.t4g.micro  # ARM-based (cheaper)

# Change storage
ALLOCATED_STORAGE=30  # GB

# Change region
AWS_REGION=us-east-1  # Virginia
```

## ðŸ§ª Testing

### 1. Test Connection

```bash
psql $DATABASE_URL -c "SELECT NOW();"
```

### 2. Run a Test Scrape

```bash
npm run dev

# In another terminal:
npm run scraper:run
```

Expected output:
```json
{
  "success": true,
  "message": "Scraped 15/15 mountains",
  "duration": 3245,
  "storage": "postgresql",
  "results": {
    "total": 15,
    "successful": 15,
    "failed": 0
  }
}
```

### 3. Check Data in Database

```bash
psql $DATABASE_URL
```

```sql
-- View latest statuses
SELECT * FROM latest_mountain_status;

-- View recent scraper runs
SELECT * FROM scraper_runs ORDER BY started_at DESC LIMIT 5;

-- Check mountain status history
SELECT * FROM get_mountain_history('baker', 7);
```

### 4. Query API Endpoints

```bash
# Get all statuses
curl http://localhost:3000/api/scraper/status

# Get specific mountain
curl http://localhost:3000/api/scraper/status?mountain=baker
```

## ðŸ“ˆ Monitoring

### View Scraper Stats

```sql
SELECT
  DATE(started_at) as date,
  COUNT(*) as runs,
  AVG(successful_count) as avg_successful,
  AVG(duration_ms) / 1000.0 as avg_duration_sec
FROM scraper_runs
WHERE started_at >= NOW() - INTERVAL '7 days'
GROUP BY DATE(started_at)
ORDER BY date DESC;
```

### Check Recent Errors

```sql
SELECT
  run_id,
  started_at,
  failed_count,
  error_message
FROM scraper_runs
WHERE status = 'failed'
ORDER BY started_at DESC
LIMIT 10;
```

### View Mountain Status Over Time

```sql
SELECT
  mountain_id,
  DATE(scraped_at) as date,
  AVG(percent_open) as avg_percent_open
FROM mountain_status
WHERE scraped_at >= NOW() - INTERVAL '30 days'
GROUP BY mountain_id, DATE(scraped_at)
ORDER BY mountain_id, date DESC;
```

## ðŸ’° Cost Estimates

### AWS Free Tier (First 12 Months)
- âœ… db.t3.micro: 750 hours/month FREE
- âœ… 20GB storage: FREE
- âœ… 20GB backups: FREE
- **Estimated: $0/month**

### After Free Tier
- db.t3.micro: ~$15/month
- 20GB storage: ~$2.30/month
- 20GB backups: ~$2.30/month
- **Estimated: ~$20/month**

### Cost Optimization Tips
1. Use db.t4g.micro (ARM) instead: ~$12/month (20% cheaper)
2. Stop instance during development: $0 when stopped
3. Use single-AZ deployment (default)
4. Enable deletion protection in production

## ðŸ” Security

### Network Security

```bash
# Allow your IP address
aws rds modify-db-instance \
  --db-instance-identifier shredders-mountain-data \
  --vpc-security-group-ids sg-xxxxxxxx  # Your security group

# Or make publicly accessible (less secure)
aws rds modify-db-instance \
  --db-instance-identifier shredders-mountain-data \
  --publicly-accessible
```

### Password Rotation

```bash
# Generate new password
NEW_PASSWORD=$(openssl rand -base64 32)

# Update RDS
aws rds modify-db-instance \
  --db-instance-identifier shredders-mountain-data \
  --master-user-password "$NEW_PASSWORD" \
  --apply-immediately

# Update .env.local and Vercel
echo "New password: $NEW_PASSWORD"
```

### SSL/TLS

Always use `sslmode=require` in connection string:

```
postgresql://user:pass@host:5432/db?sslmode=require
```

## ðŸ› ï¸ Maintenance

### Cleanup Old Data

Run weekly or monthly:

```bash
npm run db:cleanup
```

Or via cron:

```sql
-- Keep last 90 days
SELECT cleanup_old_mountain_status();
```

### Backup Database

```bash
# Manual backup
pg_dump $DATABASE_URL > backup-$(date +%Y%m%d).sql

# Restore from backup
psql $DATABASE_URL < backup-20250101.sql
```

### Monitor Performance

```sql
-- Check table sizes
SELECT
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Check index usage
SELECT
  schemaname,
  tablename,
  indexname,
  idx_scan as scans
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;
```

## ðŸ› Troubleshooting

### Connection Refused

```bash
# Check RDS status
aws rds describe-db-instances \
  --db-instance-identifier shredders-mountain-data \
  --query 'DBInstances[0].DBInstanceStatus'

# Should return: "available"
```

### SSL Error

Ensure connection string uses `sslmode=require`:

```bash
DATABASE_URL=postgresql://user:pass@host:5432/db?sslmode=require
```

### Table Not Found

Run schema setup:

```bash
npm run db:setup
```

### Out of Storage

Increase allocated storage:

```bash
aws rds modify-db-instance \
  --db-instance-identifier shredders-mountain-data \
  --allocated-storage 30 \
  --apply-immediately
```

## ðŸ“š Next Steps

1. âœ… Set up GitHub Actions for automated scraping
2. âœ… Configure Vercel environment variables
3. âœ… Test scraper with production database
4. âœ… Monitor first few scraper runs
5. â±ï¸ Set up alerting for failures (optional)

## ðŸ“ž Support

- AWS RDS Documentation: https://docs.aws.amazon.com/rds/
- Vercel Postgres: https://vercel.com/docs/storage/vercel-postgres
- PostgreSQL Documentation: https://www.postgresql.org/docs/

---

**Setup completed successfully! ðŸŽ‰**
