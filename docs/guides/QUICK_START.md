# ğŸš€ Quick Start: Automated Mountain Scraping

Get your automated mountain data scraping up and running in 15 minutes.

## âœ… Prerequisites Checklist

- [ ] AWS Account (with billing enabled)
- [ ] GitHub repository
- [ ] Vercel account and deployment
- [ ] AWS CLI installed: `brew install awscli`
- [ ] PostgreSQL client: `brew install postgresql`

## ğŸƒ 5-Step Setup

### Step 1: Install Dependencies (1 min)

```bash
cd /Users/kevin/Downloads/shredders
npm install @vercel/postgres uuid
npm install --save-dev @types/uuid
```

âœ… Already done! Packages installed.

### Step 2: Set Up AWS Database (5 min)

```bash
# Configure AWS credentials (first time only)
aws configure
# Enter: Access Key ID, Secret Key, Region (us-west-2)

# Create RDS database (takes 5-10 minutes)
npm run db:setup-aws
```

This creates:
- PostgreSQL 15.4 database (free tier eligible)
- Secure connection with SSL
- Auto-saved credentials in `.env.local`

### Step 3: Create Database Schema (1 min)

```bash
npm run db:setup
```

This creates:
- Tables for mountain status and scraper runs
- Views for latest data and statistics
- Functions for cleanup and history

### Step 4: Test Locally (2 min)

```bash
# Start dev server
npm run dev

# In another terminal, test scraper
npm run scraper:run

# Check status
npm run scraper:status
```

Expected output:
```json
{
  "success": true,
  "message": "Scraped 15/15 mountains",
  "storage": "postgresql",
  "results": {
    "successful": 15,
    "failed": 0
  }
}
```

### Step 5: Deploy and Automate (5 min)

```bash
# Add DATABASE_URL to Vercel
vercel env add DATABASE_URL production
# Paste connection string from .env.local

# Deploy to production
vercel --prod

# Set up GitHub Actions
git add .github/workflows/daily-scraper.yml
git commit -m "Add automated scraping"
git push

# Add GitHub secret
# Go to: Settings â†’ Secrets â†’ Actions â†’ New secret
# Name: APP_URL
# Value: https://your-app.vercel.app
```

## âœ¨ You're Done!

Your scraper will now run automatically:
- **10 PM PST** (evening scrape)
- **10 AM PST** (morning scrape)

## ğŸ“Š Verify It's Working

### Check GitHub Actions
1. Go to your GitHub repo â†’ **Actions** tab
2. Click **"Daily Mountain Scraper"**
3. Click **"Run workflow"** to test manually
4. Verify it completes successfully âœ…

### Check Database
```bash
psql $DATABASE_URL

SELECT COUNT(*) FROM mountain_status;
SELECT * FROM latest_mountain_status;
SELECT * FROM scraper_runs ORDER BY started_at DESC LIMIT 5;
```

### Check API
```bash
curl https://your-app.vercel.app/api/scraper/status
```

## ğŸ“š Full Documentation

- **Database Setup:** `DATABASE_SETUP.md`
- **GitHub Actions:** `GITHUB_ACTIONS_SETUP.md`
- **Scraper Details:** `src/lib/scraper/README.md`
- **Initial Setup:** `SCRAPER_SETUP.md`

## ğŸ’° Monthly Cost

**Free Tier (12 months):**
- AWS RDS: $0/month
- GitHub Actions: $0/month
- Vercel: $0/month
- **Total: $0/month** ğŸ‰

**After Free Tier:**
- AWS RDS (db.t3.micro): ~$20/month
- Everything else: $0/month
- **Total: ~$20/month**

## ğŸ¯ Success Metrics

You'll know it's working when:
- âœ… GitHub Actions runs twice daily
- âœ… 90%+ success rate
- âœ… Fresh data (< 12 hours old)
- âœ… Database growing steadily
- âœ… Both web and iOS apps show live data

## ğŸ› Troubleshooting

**Database connection failed:**
```bash
# Check DATABASE_URL is set
echo $DATABASE_URL

# Test connection
psql $DATABASE_URL -c "SELECT NOW();"
```

**Scraper returns errors:**
```bash
# Check logs
npm run dev
# Then in another terminal:
npm run scraper:run
```

**GitHub Actions not running:**
- Verify secret `APP_URL` is set
- Check workflow file syntax
- Manually trigger workflow to test

## ğŸ”„ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GitHub Actions  â”‚  Triggers twice daily (6 AM, 6 PM UTC)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP GET
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vercel App      â”‚  /api/scraper/run endpoint
â”‚ (Next.js)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Scrapes
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Resort Websites â”‚  Mt Baker, Stevens, Crystal, etc.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Data
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AWS PostgreSQL  â”‚  Stores historical data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Serves
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Web + iOS Apps  â”‚  Display live mountain conditions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“± Bonus: iOS App Integration

Your iOS app automatically gets the scraped data! No changes needed.

The API endpoints at `/api/mountains/{id}` will use the latest scraped data from the database.

## ğŸ‰ Next Steps

1. âœ… Monitor first few scraper runs
2. â±ï¸ Add Slack/Discord notifications (optional)
3. â±ï¸ Update CSS selectors for specific mountains (see `SCRAPER_SETUP.md`)
4. â±ï¸ Add more mountains to configs.ts

---

**Questions?** Check the full documentation in:
- `DATABASE_SETUP.md`
- `GITHUB_ACTIONS_SETUP.md`
- `SCRAPER_SETUP.md`
